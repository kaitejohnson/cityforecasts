# cityforecasts
R scripts to generate city-level forecasts

## Summary
This repository contains R code to generate city-level forecasts for submission to the [flu-metrocast hub](https://github.com/reichlab/flu-metrocast).
All models are exploratory/preliminary, though we will regularly update this document to describe the latest mathematical model used in the submission.

All outputs submitted to the Hub will be archived in this repository, along with additional model metadata (such as the model definition associated with a submission and details on any additional data sources used or decisions made in the submission process).
If significant changes to the model are made during submission, we will rename the model in the submission file.

Initially, we plan to fit the data from each state independently, using hierarchical partial pooling to jointly fit the cities within a state.
This initially includes producing forecast of:
|Forecast target| Location |
|-----------|-----------|
| ED visits due to ILI | New York City (5 boroughs, unknown, citywide) |
| Percent of ED visits due to flu | Texas (5 metro areas) |

We plan to use the same latent model structure for both forecast targets, modifying the observation model for count data (NYC) vs proportion data (Texas).

## Workflow
Because all data is available publicly, the forecasts generated should be completely reproducible from the specified configuration file.
We start by using the [`mvgam`](https://github.com/nicholasjclark/mvgam) package, which is a an R package that leverages both [`mgcv`](https://cran.r-project.org/web/packages/mgcv/index.html) and [`brms`](https://paulbuerkner.com/brms/) formula interface to fit Bayesian Dynamic Generalized Additive Models (GAMs).
These packages use metaprogramming to produce Stan files, and we also include the Stan code generated by the package.

To produce forecasts each week we follow the following workflow:

1. Modify the configuration file in `input/config.toml`
2. In the command line, run ` Rscript preprocess_data.R input/config.toml {index}` where index is used to track the individual model runs, which in this case, also have different pre-processing due to being from different data sources. 
3. Next run ` Rscript models.R input/config.toml {index}`
4. Lastly run `Rscript postprocess_forecasts.R input/{forecast_date}/config.toml`
5. This will populate the `output/cityforecasts/{forecast_date}` folder with a csv file formatted following the Hub submission guidelines.

Eventually, steps 2-4 will be automated with the Github Action `.git/workflows/generate_forecasts` and set on a schedule to run after 12 pm CST, corresponding to the time that the `target_data` is updated on the Hub.

## Model definition

The below describes the preliminary model used:
### Observation model
For the forecasts of counts due to ED visits, we assume a Poisson observation process

$$
y_{l,t} \sim Poisson(exp(x_{l,t}))
$$

For the forecasts of the percent of ED visits due to flu, we assume a Beta observation process on the proportion of ED visits due to flu:

```math
\begin{align}
p_{l,t} = y_{l,t} \times 100 \\
y_{l,t} \sim Beta (z_{l,t}, \phi) \\
logit(z_{l,t}) = x_{l,t}
\end{align}
```

### Latent state-space model: Dynamic hierachical GAM with independent autoregression
We model latent admissions with a hierarchical GAM component to capture shared seasonality and weekday effects and a univariate autoregressive component to capture trends in the dynamics within each location.

```math
\begin{align}
x_{l,t} \sim Normal(\mu_{l,t} + \delta_{l} x_{l,t-1},  \sigma_l)\\
\mu_{l,t} = \beta_l + f_{global,t}(weekofyear) + f_{l,t}(weekofyear) \\
\beta_l \sim Normal(\beta_{global}, \sigma_{count}) \\
\sigma_{count} \sim exp(0.33) \\
\delta_l \sim Normal(0.5, 0.25) T[0,1] \\
\sigma \sim exp(1) \\
\end{align}
```

For the NYC data, we have count data on a daily scale so we add in a weekday component
```math
\mu_{l,t} =  \beta_l + f_{global,t}(week) + f_{l,t}(week) + f_{global,t}(wday)
```
And since $\beta_{global}$ represents the intecept on the count scale, we place a prior on it using the mean observed count across the historical data:

$$
\beta_{global} \sim Normal(log(\frac{\sum_{l=1}^L \sum_{t=1}^T y_{l,t}}{N_{obs}}), 1) \\
$$

where $N_obs$ is the number of observations of $y_{l,t}$. 

For the TX data, $\beta_{global}$ represents the intercept as a proportion, so we use:

$$
\beta_{global} \sim Normal(logit(\frac{\sum_{l=1}^L \sum_{t=1}^T y_{l,t}}{N_{obs}}), 1) \\
$$



For the NYC data, we have daily data so $t$ is measured in days, whereas for the Texas data, $t$ is measured in weeks.

## Replacement models 
The above model estimates a hierarchical dynamic GAM, which contains both a GAM component and an autoregressive component. 
We can additionally fit a more traditional hierarchical GAM (with no autoregression but with tensor product splines to jointly estimate across location and time) as well as a vector ARIMA without a spline component. Eventually, we can also mash everything together and estimate a hierarchical GAM with a multivariante vector autoregression. These will be areas of future work. 

### Dynamic Hierarchical Vector Autoregression 
```math
\begin{align}
x_{l,t} \sim MVNormal(\mu_{l,t} + A \times(X_{l,t-1} - \mu_{l, t-1}),  \Sigma)\\
\mu_{l,t} = \beta_{l,season} + f_{global,t}(week) + f_{l,t}(week) \\
\beta_{l,season} \sim Normal(\beta_{global}, \sigma_{count}) \\
\sigma_{count} \sim exp(0.33) \\
A \in P(\mathbb{R})\\
P \sim Normal(0, 0.5) T[-1,1] \\
\Sigma = \sigma \times C \times \sigma \\
\sigma \sim Beta(3,3) \\
C \sim LKJcorr(2) \\
\end{align}
```
We could optionally add a time-varying seasonality component (on seasons, not years) so that we learn a global effect of the shifting of seasons over time, however, because the current season will always only be partially observed, this might not be ideal. 

Additionally, for the data where we do have daily data, we can additionally add a random effect for the observations of partial weeks, which would look something like:
```math
\begin{align}
x_{l,t} \sim MVNormal(\mu_{l,t} + A \times (X_{l,t-1} - \mu_{l, t-1}),  \Sigma)\\
\mu_{l,t} = \beta_{l,season} + f_{global,t}(week) + f_{l,t}(week) + I_{pw}\alpha_{l,season} \\
\beta_{l,season} \sim Normal(\beta_{global}, \sigma_{count}) \\
\alpha_l \sim Normal(log(3/7), 0.2) T[-\inf, 0] \\
\sigma_{count} \sim exp(0.33) \\
 A \in P(\mathbb{R})\\
P \sim Normal(0, 0.5) T[-1, 1]\\
\Sigma = \sigma \times C \times \sigma \\
\sigma \sim Beta(3,3) \\
C \sim LKJcorr(2) \\
\end{align}
```
Where $\alpha_{l, season}$ is the location and season-specific random effect on the indicator variable $I_{pw}$ which is set to 0 if the observation is from a full week and 1 if the observation is from a partial week. The prior is chosen based on the typicaly proprtion of the days being measured in the partial week. We restrict the value of $\alpha_{l,season}$ to negative values, it should always scale the expected counts by less than 1.  

## Preliminary Analysis Plan
The preliminary goal will be to use this model to investigate the value of partially pooling data across locations. 
Internally, we will produce real-time forecasts for the cities in the forecast Hub using the partially pooled version of the model described above. We will then compare to a model that fits each location independently (using the same GAM structure and autoregression, just with no global effects or interacting autoregulation coefficients, as well as a model that fits only to the fully pooled data (so one location). 

We will assess the forecast performance of each variation of the model, both comparing between the models within this dynamic GAM autoregressive structure, and between other models submitted to the Hubs. To ensure our results are not a byproduct of the chosen model structure here, we will also produce forecasts from either only VAR models and only dynamic GAM models, also under the different pooling conditions. 

Additional analysis components:
- consider adding some different variations of handling seasonality including:
   - time-varying seasonality "drift" which assumes this season will be most similar to previous seasons
   - regress towards average of past historical seasons
   - something that has a negative feedback in seasonal magnitude (e.g if last season big/early, this season small/late)
   - learn a MVN across locations and seasons

